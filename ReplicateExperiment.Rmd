---
title: "Replicate_Experiment"
author: "Jeffrey R. Weidner, PhD"
date: "2023-07-24"
output: html_document
---

# Replicate-Experiment

## Introduction

Statistical process control (SPC) is the use of statistical methods to monitor and control the quality of processes. Though the tools were initially developed and used for manufacturing, they have found applications in many other areas where outcomes can be measured and quality is valued, including scientific laboratories. Some of the most common SPC tools include Run Charts, Control Charts, Experimental Design, and Replicate Experiment.

The Replicate-Experiment is based on [Bland-Altman difference analysis^1^](https://www-users.york.ac.uk/~mb55/meas/ba.pdf) between two sets of measurements and is used to assess either **repeatability** or **reproducibility**. These terms have specific meanings in SPC. **Repeatability** is the ability to produce comparable results within a group of conditions (e.g. people, intstruments, reagents, ...), while **reproducibility** refers to the ability to replicate results across different conditions (e.g. between two assayers or pieces of equipment). Initially repeatability should be demonstrated by having a scientist test the same set of samples with 2 independently prepared sets of working reagents/cells (note: these experiments can be performed at the same time or on different days) on the same set of equipment. Once repeatability has been established, reproducibility can be determined between different assayers, pieces of equipment, or lots of reagents or cells.

## Rationale

Replicate-Experiment studies are used to formally evaluate the *within-run* assay variability and formally compare the new assay to the existing (old) assay. They also allow a preliminary assessment of the *overall* or *between-run* assay variability, but two runs are not enough to adequately assess overall variability. Post-Production monitoring, such as [Retrospective MSR^2^](https://www.ncbi.nlm.nih.gov/books/NBK169432/) analysis and Control Charts are used to formally evaluate the overall variability in the assay. Note that the Replicate-Experiment study is a diagnostic and decision tool used to establish that the assay is ready to go into production by showing that the endpoints of the assay are reproducible over a range of potencies. It is not intended as a substitute for post-production monitoring or to provide an estimate of the overall Minimum Significant Ratio (MSR).

It may seem counter-intuitive to call the differences between two independent assay runs as *within-run* variability. However, the terminology results from how assay runs are defined. Experimental variation is categorized into two distinct components: *between-run* and *within-run* sources. Consider the following examples:

-   If there is variation in the concentrations of buffer components between 2 runs, then the assay results could be affected. However, assuming that the same buffer is used with all compounds within one run, each compound will be equally affected and so the difference will only show up when comparing one run to another run, i.e. in two runs, one run will appear higher on average than the other run. This variation is called *between-run* variation.

-   If the concentration of a compound in the stock plate varies from the target concentration then all wells where that compound is used will be affected. However, wells used to test other compounds will be unaffected. This type of variation is called *within-run* as the source of variation affects different compounds in the same run differently.

-   Some sources of variability affect both within- and between-run variation. For example, if assay cells are plated and then incubated for 24-72 hours to achieve a target cell density taking into account the doubling time of the cells. If the doubling time equals the incubation time, and the target density is 30,000 cells/well, then 15,000 cells/well are plated. But even if exactly 15,000 cells are placed in each well there won't be exactly 30,000 cells in each well after 24 hours. Some will be lower and some will be higher than the target. These differences are *within-run* as not all wells are equally affected. But also suppose in a particular run only 13,000 cells are initially plated. Then the wells will on average have fewer than 30,000 cells after 24 hours, and since all cells are affected this is *between-run* variation. Thus cell density has both *within*- and *between*-run sources of variation.

The total variation is the sum of both sources of variation. When comparing two compounds across runs, one must take into account both the *within-run* and *between-run* sources of variation. But when comparing two compounds in the same run, one must only take into account the *within-run* sources, since, by definition, the *between-run* sources affect both compounds equally.

In a Replicate-Experiment study the *between-run* sources of variation cause one run to be on average higher than the other run. However, it would be very unlikely that the difference between the two runs were exactly the same for every compound in the study. These individual compound "differences from the average difference" are caused by the *within-run* sources of variation. The higher the within-run variability the greater the individual compound variation in the assay runs.

**The analysis approach used in the Replicate-Experiment study is to estimate and factor out between-run variability, and then estimate the magnitude of within-run variability.**

## Experimental Procedure

The Replicate-Experiment is intended to be easy to execute with a modest resource commitment. Most executions can be performed with 2-4 assay plates. It is most commonly run in the potency mode, though it can be run in efficacy mode to gain a better understanding of assay variability across the dynamic range of the assay or facilitate the interpretation of screening results.

The potency mode is ideally run with 20-30 active compounds with a broad range of potencies and the potencies should be well spaced across the range of potencies. If this number of active compounds is not available, then it can be run with a small set of compounds in replicate, with each replicate treated as an independent sample (e.g. 5 compounds with 5 replicate dilutions).

In the efficacy mode, it is particularly important to have samples where the activity spans the dynamic range of the assay. Often the variability of measurements will not be constant across the dynamic range of the assay. For this reason, efficacy studies should not be conducted with random screening plates, since most compounds will be inactive and skew the assessment. It may be simpler to use a small number of active compounds in a. dilution series, as if for a potency determination, but treat each dilution as an independent sample for the efficacy analysis. This will ensure that the data cover the entire dynamic range of the assay with just a few compounds.

Initially **repeatability** should be demonstrated with identical compounds tested with 2 independently prepared sets of reagents/cells. Once **repeatability** has been demonstrated for a protocol, **reproducibility** can be measured by comparing the same compounds run in the new and the old assay formats. **Reproducibility** can be used to validate minor assay changes such as new assayers, equipment substitutions, or changes to lots of reagents or cells.

## Data Analysis

The statistical analysis assumes that any measurement errors are normally distributed. While this is true for efficacy data, potency data is log-normal. This means that potency data must first be transformed to their log~10~ values before the analysis. After that transformation the analysis methods are the same:

1.  For each pair of measurements calculate the mean = (meas1 + meas2)/2 and the difference = (meas1 - meas2). *Note when these values are transformed back to the linear scale for potency data they will generate the geometric mean and the ratio, since log(meas1) - log(meas2) = log(meas1/meas2).*

2.  Calculate the mean ($\bar{d}$) and standard deviation (sd) for the set of the difference values.

3.  Calculate the difference limits $DLs = \bar{d}\pm2sd/\sqrt{n}$ Where n is the number of compounds tested. This is the 95% confidence interval for the difference.

4.  Calculate aggreement limits $ALs = \bar{d}\pm2sd$ Most of the compound differences (\~95%) should fall within these limits.

5.  Calculate the Minimum Significant Difference $MSD = 2sd$ This is the minimum difference between two compounds that is statistically significant.

6.  For potency data all statistics are transformed back to linear scale and differences become ratios (e.g. Minimum Significant Ratio $MSR = 10^{MSD}$).

Fortunately all of this analysis can be done with the Replicate-Experiment web tool (insert link). Just create a .csv file of your data in any spreadsheet program and upload it to the tool for analysis.

```{r setup, echo=FALSE, message=FALSE}

# Functionalized version of msx.R

library(tidyverse)
library(patchwork)
library(ggrepel)

# Test data generation functions ---------------------------------------
set.seed(12252023) #For reproducible examples 

# Generate Potency test data for Replicate Experiment
msr_data <- function(SmplNum, TstMSR, Shift = 1) {
  
  TstSD <- log10(TstMSR) / 2
  Shift <- log10(Shift)
  Sample <- sample(c(1000000 : 9999999), SmplNum)
  TrueMeas <- runif(SmplNum, -3, 3)
  ExpData <- map(TrueMeas, \(m) rnorm(2, m, TstSD))
  
  TstData <- tibble(Sample, ExpData) %>% 
    unnest_wider(col = ExpData, names_sep = '_') %>% 
    rename(Exp1 = ExpData_1, Exp2 = ExpData_2) %>% 
    mutate(Exp2 = Exp2 + Shift,
           across(-Sample, ~ 10 ^ .x))
}

# Generate Efficacy test data for Replicate Experiment- consistent SD
msd_data <- function(SmplNum, TstMSD, Shift = 0) {
  
  TstSD <- TstMSD /2
  Sample <- sample(c(1000000 : 9999999), SmplNum)
  TrueMeas <- runif(SmplNum, -20, 120)
  ExpData <- map(TrueMeas, \(m) rnorm(2, m, TstSD))
  
  TstData <- tibble(Sample, ExpData) %>% 
    unnest_wider(col = ExpData, names_sep = '_') %>% 
    rename(Exp1 = ExpData_1, Exp2 = ExpData_2) %>% 
    mutate(Exp2 = Exp2 + Shift)
}

# Generate Efficacy test data for Replicate Experiment - consistent cv
msd_cv_data <- function(SmplNum, cv, Shift = 0) {

  Sample <- sample(c(1000000 : 9999999), SmplNum)
  TrueMeas <- runif(SmplNum, -10, 110)
  ExpData <- map(TrueMeas, \(m) rnorm(2, m, abs(cv*m)))
  
  TstData <- tibble(Sample, ExpData) %>% 
    unnest_wider(col = ExpData, names_sep = '_') %>% 
    rename(Exp1 = ExpData_1, Exp2 = ExpData_2) %>% 
    mutate(Exp2 = Exp2 + Shift)
}

# Generate inactive efficacy test data for Replicate Experiment
msd_inact <- function(SmplNum, TstMSD, Shift = 0) {
  
  TstSD <- TstMSD /(2 * sqrt(2))
  Sample <- sample(c(1000000 : 9999999), SmplNum)
  TrueMeas <- runif(SmplNum, -20, 20)
  ExpData <- map(TrueMeas, \(m) rnorm(2, m, TstSD))
  
  TstData <- tibble(Sample, ExpData) %>% 
    unnest_wider(col = ExpData, names_sep = '_') %>% 
    rename(Exp1 = ExpData_1, Exp2 = ExpData_2) %>% 
    mutate(Exp2 = Exp2 + Shift)
}

# Generate active efficacy test data for Replicate Experiment
msd_act <- function(SmplNum, TstMSD, Shift = 0) {
  
  TstSD <- TstMSD /(2 * sqrt(2))
  Sample <- sample(c(1000000 : 9999999), SmplNum)
  TrueMeas <- runif(SmplNum, 50, 120)
  ExpData <- map(TrueMeas, \(m) rnorm(2, m, TstSD))
  
  TstData <- tibble(Sample, ExpData) %>% 
    unnest_wider(col = ExpData, names_sep = '_') %>% 
    rename(Exp1 = ExpData_1, Exp2 = ExpData_2) %>% 
    mutate(Exp2 = Exp2 + Shift)
}

# Concordance Correlation V. Devanarayan --------------------------------
conc.corr <- function(x, y, alpha = 0.05)
{
  qval <- qnorm(1. - alpha/2.)
  N <- length(x)
  lm.xy <- lm(y ~ x)
  Intercept <- c(lm.xy$coeff[1.])
  Slope <- c(lm.xy$coeff[2.])
  A <- 2. * Slope * var(x)
  B <- var(y) + var(x)
  C <- (Intercept + (Slope - 1.) * (mean(x)))^2.
  rho <- A/(B + C)
  U <- (mean(x) - mean(y))^2./(sqrt(var(x)) * sqrt(var(y)))
  # cat("Zhat = ",Zhat,"\n")
  # cat("D=",D,"\n")
  # cat("E=",E,"\n")
  Zhat <- 0.5 * logb((1. + rho)/(1. - rho))
  D <- (4. * rho^3. * (1. - rho) * U)/(rho * (1. - rho^2.)^2.)
  E <- (2. * rho^4. * U^2.)/(rho^2. * (1. - rho^2.)^2.)
  # cat("var.Zhat=",var.Zhat,"\n")
  var.Zhat <- 1./(N - 2.) * (D - E)
  lowlim <- Zhat - qval * sqrt(var.Zhat)
  # cat(lowlim,upplim,"\n")
  # Now transform back:
  upplim <- Zhat + qval * sqrt(var.Zhat)
  lowlim <- (exp(2. * lowlim) - 1.)/(1. + exp(2. * lowlim))
  upplim <- (exp(2. * upplim) - 1.)/(1. + exp(2. * upplim))
  lowlim <- max(lowlim, -1.)
  upplim <- min(upplim, 1.)
  list(rho = rho, lowlim = lowlim, upplim = upplim)
}

# Replicate-Experiment Analysis Functions -------------------------------

# Summary Stats
repexp.stats <- function(df) {
  ConcCorr <- conc.corr(df$Exp1, df$Exp2)
  rho <- as.double(ConcCorr[1])  
  
  summarise(df,
            n = n(), 
            t = qt(0.975, n - 1),
            MeanDiff = mean(Difference),
            StdDev = sd(Difference),
            MSD = 2 * StdDev,
            UDL = MeanDiff + (t * StdDev/sqrt(n)),
            LDL = MeanDiff - (t * StdDev/sqrt(n)),
            ULSA = MeanDiff + (t * StdDev),
            LLSA = MeanDiff- (t * StdDev)) %>% 
    mutate(r = rho) %>% 
    select(-t, -StdDev)
}

#Mean Difference Plot
mdplot <- function(Data, Stats) {
  MSDLabel <- paste("MSD =", format(Stats$MSD, digits = 2))
  
  ggplot(Data, aes(x = Mean, y = Difference)) +
    geom_point(shape = Data$Outlier, size = 3) +
    geom_text(aes(label = Label), na.rm = TRUE, hjust = -.5) +
    geom_hline(yintercept = Stats$MeanDiff, color = "mediumblue") +
    geom_hline(yintercept = 0, color = "black") +
    geom_hline(yintercept = Stats$UDL, color = "mediumblue", linetype = "dashed") +
    geom_text(x = 1, y = Stats$UDL, label = "Upper Difference Limit", color = "mediumblue", vjust = -0.3) +
    geom_hline(yintercept = Stats$LDL, color = "mediumblue", linetype = "dashed") +
    geom_text(x = 1, y = Stats$LDL, label = "Lower Difference Limit", color = "mediumblue", vjust = 1) +
    geom_hline(yintercept = Stats$ULSA, color = "#D55E00", linetype = "dotdash") +
    geom_text(x = 1, y = Stats$ULSA, label = "Upper Limit of Aggreement", color = "#D55E00", vjust = -0.3) +
    geom_hline(yintercept = Stats$LLSA, color = "#D55E00", linetype = "dotdash") + 
    geom_text(x = 1, y = Stats$LLSA, label = "Lower Limit of Aggreement", color = "#D55E00", vjust = 1) +
    theme_minimal() +
    labs(title="Difference vs Mean Efficacy",
         subtitle = MSDLabel,
         x ="Mean Efficacy", y = "Difference")
}

# Mean Ratio Plot
mrplot <- function(Data, Stats) {
  MSRLabel <- paste("MSR =", Stats$MSR)
  
  ggplot(Data, aes(x =  GeometricMean, y = Ratio)) +
    geom_point(shape = Data$Outlier, size = 3) +
    geom_text(aes(label = Label), na.rm = TRUE, hjust = -.5) +
    geom_hline(yintercept = Stats$MeanRatio, color = "mediumblue") +
    geom_hline(yintercept = 1, color = "black") +
    geom_hline(yintercept = Stats$UDL, color = "mediumblue", linetype = "dashed") +
    geom_text(x = 1, y = log10(Stats$UDL), label = "Upper Ratio Limit", color = "mediumblue", vjust = -0.3) +
    geom_hline(yintercept = Stats$LDL, color = "mediumblue", linetype = "dashed") +
    geom_text(x = 1, y = log10(Stats$LDL), label = "Lower Ratio Limit", color = "mediumblue", vjust = 1) +
    geom_hline(yintercept = Stats$ULSA, color = "#D55E00", linetype = "dotdash") +
    geom_text(x = 1, y = log10(Stats$ULSA), label = "Upper Limit of Aggreement", color = "#D55E00", vjust = -0.3) +
    geom_hline(yintercept = Stats$LLSA, color = "#D55E00", linetype = "dotdash") + 
    geom_text(x = 1, y = log10(Stats$LLSA), label = "Lower Limit of Aggreement", color = "#D55E00", vjust = 1) +
    theme_minimal() +
    scale_x_continuous(trans='log10') + 
    scale_y_continuous(trans='log10') +
    labs(title="Potency Ratio vs Geometric Mean Potency",
         subtitle = MSRLabel,
         x ="Geometric Mean Potency", y = "Ratio")
}

# R1R2 Plot - Run1/Run2 with correlation.

r1r2plot <- function(Data, Stats) {
  ggplot(Data,aes(x = Exp1, y = Exp2)) +
    geom_point() +
    geom_text(aes(label = Label), na.rm = TRUE, hjust = -.5) +
    geom_smooth(method = "lm", se = TRUE, linetype = "dashed", linewidth = 2) +
    geom_abline(slope = 1) +
    labs(title = "Correlation Exp1 vs Exp2",
         subtitle = paste("Concordance Correlation r =", Stats$r),
         x = "Exp1",
         y = "Exp2") +
    theme_minimal()
}

#Replicate-Experiment Efficacy --------------------------------------------

repexp.efficacy<- function(df) {
  
RepExp_Data <- df %>% 
    mutate(Mean = (Exp1 + Exp2) / 2,
           Difference = Exp1 - Exp2)
 
RepExp_Stats <- repexp.stats(RepExp_Data) %>% 
  mutate(across(-n, \(x) signif(x, digits = 3)))

# Flag data pairs with potential outliers (MeasDiff outside of limits of agreement)

RepExp_Data <- RepExp_Data %>% 
  mutate(Outlier = Difference > RepExp_Stats$ULSA | Difference < RepExp_Stats$LLSA,
         Label = if_else(Outlier, Sample, NA))

MeanDifferencePlot <- mdplot(RepExp_Data, RepExp_Stats)

R1R2CorrelationPlot <- r1r2plot(RepExp_Data,RepExp_Stats)

list(Data = RepExp_Data, Stats = RepExp_Stats, MDPlot = MeanDifferencePlot, CorrPlot = R1R2CorrelationPlot)
}

# Replicate-Experiment Potency -------------------------------

repexp.potency <- function(df) {
  
  RepExp_Data <- df %>% 
    mutate(across(-Sample, log10),
      Mean = (Exp1 + Exp2) / 2,
           Difference = Exp1 - Exp2)
  
  RepExp_Stats <- repexp.stats(RepExp_Data)
  
  # Flag data pairs with potential outliers (MeasDiff outside of limits of agreement)
  
  RepExp_Data <- RepExp_Data %>% 
    mutate(Outlier = Difference > RepExp_Stats$ULSA | Difference < RepExp_Stats$LLSA,
           Label = if_else(Outlier, Sample, NA),
           across(-c(1, 6, 7), ~10 ^ .x),
           across(-c(1, 6, 7),  \(x) signif(x, digits = 3))) %>% 
    rename(GeometricMean = Mean, Ratio = Difference)
  
  RepExp_Stats <- RepExp_Stats %>% 
    rename(MSR = MSD,
           MeanRatio = MeanDiff,
           URL = UDL,
           LRL = LDL) %>% 
    mutate(across(-c(1, 8), ~10 ^ .x),
           across(-c(1), \(x) signif(x, digits = 3)))
  
  MeanRatioPlot <- mrplot(RepExp_Data, RepExp_Stats)
  
  R1R2CorrelationPlot <- r1r2plot(RepExp_Data,RepExp_Stats) +
    scale_x_continuous(trans='log10') + 
    scale_y_continuous(trans='log10')
  
  list(Data = RepExp_Data, Stats = RepExp_Stats, MRPlot = MeanRatioPlot, CorrPlot = R1R2CorrelationPlot)
}
```

## Scenarios

### Efficacy

Efficacy measurements can be made with raw data from a detector or data which has been normalized (e.g. % Activity) using plate controls. Normalized data is preferred, since it provides biological context and facilitates comparisons across the lifetime of an assay.

A replicate-experiment can be used to determine the minimum difference in measured activities for 2 measurements to be statistically different ( $p \leq 0.05$). This is known as the **Minimum Significant Difference (MSD)**. The following scenario illustrates how MSD can be determined using just 2 384-well assay plates. Each plate contains the same 320 samples with activities that cover the full dynamic range of the assay and 64 control wells for the normalization. The plates are tested with independently prepared reagents/cells.

```{r efficacy320, echo=FALSE}

Efficacy320Data <- msd_data(SmplNum = 320, TstMSD = 30)
Efficacy320MSD <- repexp.efficacy(Efficacy320Data)
Efficacy320Data
```

Here are the data from the first 10 samples. The data is simply the sample identifiers (numeric or character) and the measured activity values for the 2 experiments. This is all that is needed to upload for analysis. The first step of the analysis is to simply compute the average activity and the difference (Exp1 - Exp2) for each sample.

```{r echo=FALSE}
MSD320Calc <- Efficacy320MSD$Data %>% 
  select(1:5)
MSD320Calc

MDPlot <- ggplot(MSD320Calc, aes(x = Mean, y = Difference)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  theme_classic()

DiffDist <- ggplot(MSD320Calc, aes(x = Difference)) +
  geom_histogram() +
  theme_classic()

MSD320Plots <- MDPlot / DiffDist + plot_annotation(tag_levels = 'A')
MSD320Plots
```

The mean represents an estimate of the true activity for a sample, while the difference provides an estimate of the variability in the measurements. Plotting the mean on the x-axis and difference on the y-axis (A) provides an overview of the variability in the data across the activity range of the assay. The distribution of the difference values (B) appears to be normal and centered on 0. The mean value of the differences (Mean Difference = `r mean(MSD320Calc$Difference)`) is an indicator of *between run* variability, while the standard deviation of those differences (SD = `r sd(MSD320Calc$Difference)`) can be used to calculate the Difference Limits (DLs) or the SEM for this measurement, as well as the MSD and Limits of Statistical Agreement (LSAs, 99% confidence interval). Data outside of those LSA's are tagged as potential outliers and highlighted in the plots.

```{r echo = FALSE}

Efficacy320MSD$Stats
Efficacy320MSD$MDPlot
```

A correlation plot of the two data sets is also generated.

```{r echo=FALSE, message=FALSE}
Efficacy320MSD$CorrPlot
```

The data is also returned with any potential outliers identified.

```{r echo=FALSE}
Data <- Efficacy320MSD$Data %>% 
  arrange(Label) %>% 
  select(-Label)
Data
```

### Efficacy - Variability differences across assay range.

The variability of assay efficacy measurements may differ significantly across the dynamic range of the assay. Consider an assay that has a consistent cv (10%) across the range of activity measurements. That might produce the following results

```{r echo=FALSE, message=FALSE}

EfficacycvData <- msd_cv_data(SmplNum = 320, cv = 0.1)
EfficacycvMSD <- repexp.efficacy(EfficacycvData)

EfficacycvMSD$Stats
EfficacycvMSD$MDPlot
EfficacycvMSD$CorrPlot

```

Both the Mean-Difference and Correlation plots show more variability in the data as a function if increasing activity. In the case of a signal decrease assay the variability would be increased at lower % activity levels. Assays which show this behavior should not use the replicate experiment as a general measurement of uncertainty in the assay. However Replicate-Experiments with the vehicle and active controls analyzed separately can be useful that the assay is repeatable and helping to define limits for activity. In fact, the numerator in the Z' equation is the difference between the Limits of Agreement for the two controls.

$$
Z' = \frac{(\overline{Max} - 3 * sd_{Max})-(\overline{Min} + 3 * sd_{Min})}{\overline{Max} - \overline{Min}}
$$

### Potency

As discussed in the Data Analysis section, potency measurements must be converted to their log~10~ values for the statistical analysis, so that the variability will be normally distributed. However this conversion and the antilog back to the linear scale do affect the interpretation of the data. Since $log(A) - log(B) = log(A/B)$, the differences from the statistical analysis become ratios when converted back to the linear scale. Thus uncertainties and confidence limits for potency values should be interpreted as fold-differences from the measured value. Additionally, since any units of measure will cancel out in the ratio, they are consistent across the measurement range, unlike efficacy measurements.

Consider the following data set with 32 compounds with a range of potencies. The dose response plates are prepared and tested in 2 independent experiments, as with the efficacy experiment, but now the potency values (e.g. AC~50~) are compared.

```{r echo=FALSE}
Pot32Data <- msr_data(32, 3)
Pot32MSR <- repexp.potency(Pot32Data)
Pot32Data <- Pot32Data %>% 
  mutate(across(-Sample, \(x) signif(x, digits = 3)))
Pot32Data
```

Now, when the analysis is run, the data are returned with the geometric mean estimating the true potency and the ratio of the potencies as a measure of the variability in the measurements.

```{r echo=FALSE}
Pot32MSR$Data
```

Similarly, the statistics now use ratios for the uncertainty measurements. The Mean Ratio should be close to 1 if there is no significant *between-run* variability. The Mean Ratio limits represent the SEM for the Mean Ratio. The Upper and Lower Aggreement Limits are ratios representing 99% confidence intervals. The correlation coefficient, r, is also provided.

```{r echo=FALSE}
Pot32MSR$Stats
```

The Mean-Ratio plot is equivalent to the Mean-Difference plot for efficacy data and is interpreted similarly. The geometric mean potency is plotted on the x-axis and the ratio values on the y-axis with log10 scales. Ideally the x-axis values should cover at least 3 logs or the full range of available potencies. The ratio values should be evenly distributed across the Mean Ratio line.

```{r echo=FALSE}
Pot32MSR$MRPlot
```

The Correlation plot uses the x-axis for the first experiment and the y-axis for the second experiment with log scales. A reference line indicating unity along with the correlation line and correlation coefficient.

```{r echo=FALSE, message=FALSE}
Pot32MSR$CorrPlot
```

### Mean Ratio or Mean Difference Shift

The Mean Ratio and Mean Difference can reveal systematic differences between the two experiments. If the differences in the assay measurements are random, then the Mean Ratio should be equal or close to 1 and the Mean Difference should be equal of close to 0. If the ratio/dose limits do not include those values, that is a strong indication that there is a cause for the difference. This cause may be something obvious (e.g. such as different assayers, reagent lots, instruments, ...) or they may require some investigation (cell line drift, instrument settings or calibration, ...).

```{r echo=FALSE, message=FALSE}
PotShiftData <- msr_data(SmplNum = 32, TstMSR = 2.7, Shift = 1.5)
PotShiftMSR <- repexp.potency(PotShiftData)
PotShiftMSR$Data
PotShiftMSR$Stats
PotShiftMSR$MRPlot
PotShiftMSR$CorrPlot

```
