[
  {
    "objectID": "RepExpDoc.html",
    "href": "RepExpDoc.html",
    "title": "Replicate Experiment",
    "section": "",
    "text": "Statistical process control (SPC) is the use of statistical methods to monitor and control the quality of processes. Though the tools were initially developed and used for manufacturing, they have found applications in many other areas where outcomes can be measured and quality is valued, including scientific laboratories. Some of the most common SPC tools include Run Charts, Control Charts, Experimental Design, and Replicate Experiment.\nThe Replicate-Experiment (Eastwood et al. 2006a) is based on Bland-Altman difference analysis (Bland and Altman 1986) between two sets of measurements and is used to assess either repeatability or reproducibility. These terms have specific meanings in SPC. Repeatability is the ability to produce comparable results within a group of conditions (e.g. people, instruments, reagents, …), while reproducibility refers to the ability to replicate results across different conditions (e.g. between two assayers or pieces of equipment). Initially repeatability should be demonstrated by having a scientist test the same set of samples with 2 independently prepared sets of working reagents/cells (note: these experiments can be performed at the same time or on different days) on the same set of equipment. Once repeatability has been established, reproducibility can be determined between different assayers, pieces of equipment, or lots of reagents or cells.\n\n\n\nReplicate-Experiment studies (Iversen et al. 2012) are used to formally evaluate the within-run assay variability and formally compare a new assay to the existing (old) assay. They also allow a preliminary assessment of the overall or between-run assay variability, but two runs are not enough to adequately assess overall variability. Post-production monitoring, such as Retrospective MSR (Haas et al. 2017) analysis and Control Charts (Beck et al. 2017) are used to formally evaluate the overall variability in the assay. Note that the Replicate-Experiment study is a diagnostic and decision tool used to establish that the assay is ready to go into production by showing that the endpoints of the assay are repeatable over a range of values. It is not intended as a substitute for post-production monitoring or to provide an estimate of the overall Minimum Significant Ratio (MSR).\nIt may seem counter-intuitive to call the differences between two independent assay runs as within-run variability. However, the terminology results from how assay runs are defined. Experimental variation is categorized into two distinct components: between-run and within-run sources. Consider the following examples:\n\nIf there is variation in the concentrations of buffer components between 2 runs, then the assay results could be affected. However, assuming that the same buffer is used with all compounds within one run, each compound will be equally affected and so the difference will only show up when comparing one run to another run, i.e. in two runs, one run will appear higher on average than the other run. This variation is called between-run variation.\nIf the concentration of a compound in the stock plate varies from the target concentration then all wells where that compound is used will be affected. However, wells used to test other compounds will be unaffected. This type of variation is called within-run as the source of variation affects different compounds in the same run differently.\nSome sources of variability affect both within- and between-run variation. For example, if assay cells are plated and then incubated for 24-72 hours to achieve a target cell density taking into account the doubling time of the cells. If the doubling time equals the incubation time, and the target density is 30,000 cells/well, then 15,000 cells/well are plated. But even if exactly 15,000 cells are placed in each well there won’t be exactly 30,000 cells in each well after 24 hours. Some will be lower and some will be higher than the target. These differences are within-run as not all wells are equally affected. But also suppose in a particular run only 13,000 cells are initially plated. Then the wells will on average have fewer than 30,000 cells after 24 hours, and since all cells are affected this is between-run variation. Thus cell density has both within- and between-run sources of variation.\n\nThe total variation is the sum of both sources of variation. When comparing two compounds across runs, one must take into account both the within-run and between-run sources of variation. But when comparing two compounds in the same run, one must only take into account the within-run sources, since, by definition, the between-run sources affect both compounds equally.\nIn a Replicate-Experiment study, the between-run sources of variation cause one run to be on average higher than the other run. However, it would be very unlikely that the differences between the two runs were exactly the same for every compound in the study. These individual compound “differences from the average difference” are caused by the within-run sources of variation. The higher the within-run variability the greater the individual compound variation in the assay runs.\nThe analysis approach used in the Replicate-Experiment study is to estimate and factor out between-run variability, and then estimate the magnitude of within-run variability.\n\n\n\nThe Replicate-Experiment is intended to be easy to execute with a modest resource commitment. Most executions can be performed with 2-4 assay plates. It is most commonly run in the potency mode, though it can be run in efficacy mode to gain a better understanding of assay variability across the dynamic range of the assay or facilitate the interpretation of screening results.\nThe potency mode is ideally run with 20-30 active compounds with a broad range of potencies and the potencies should be well spaced across the range of values. If this number of active compounds is not available, then it can be run with a smaller set of compounds in replicate, with each replicate treated as an independent sample (e.g. 5 compounds with 5 separate replicate dilutions).\nIn the efficacy mode, it is particularly important to have samples where the activity spans the dynamic range of the assay. Often the variability of measurements will not be constant across the dynamic range of the assay. For this reason, efficacy studies should not be conducted with random screening plates, since most compounds will be inactive and could skew the assessment. It may be simpler to use a small number of active compounds in a dilution series, as if for a potency determination, but treat each dilution as an independent sample for the efficacy analysis. This will ensure that the data cover the entire dynamic range of the assay with just a few compounds.\nInitially repeatability should be demonstrated with identical compounds tested with 2 independently prepared sets of reagents/cells. Once repeatability has been demonstrated for a protocol, it is ready for routine testing. The assay should be monitored to ensure it behaves as validated. This can include any or all of the following methods:\n\nControl charting reference compound(s) (Beck et al. 2017).\nRetrospective MSR analysis (Haas et al. 2017).\nperiodic retests if the samples used in previous replicate experiments to compare to previous data (Eastwood et al. 2006b).\n\nThe replicate experiment is also used to validate minor assay changes such as new assayers, equipment substitutions, or changes to lots of reagents or cells. In this case, the data from identical samples is compared in the current and new formats. Historical data can be used for the current format, as long as it comes from a single experiment.\n\n\n\nThe statistical analysis assumes that any measurement errors are normally distributed. While this is true for efficacy data, potency data is log-normal. This means that potency data must first be transformed to their log10 values before the analysis. After that transformation the analysis methods are the same:\n\nFor each pair of compound measurements, calculate the mean = (meas1 + meas2)/2 and the difference = (meas1 - meas2). Note when these values are transformed back to the linear scale for potency data they will generate the geometric mean and the ratio, since log(meas1) - log(meas2) = log(meas1/meas2).\nCalculate the mean (\\(\\bar{d}\\)) and standard deviation (sd) for the set of the difference values.\nCalculate the difference limits \\(DLs = \\bar{d}\\pm2sd/\\sqrt{n}\\) where n is the number of compounds tested. This is the 95% confidence interval for the mean difference.\nCalculate limits of agreement \\(LSAs = \\bar{d}\\pm2sd\\). Most of the individual compound differences (~95%) should fall within these limits.\nSamples outside the agreement limits are flagged as outliers. Since these values are 2sd outliers, it is expected that 5% of the data to be flagged if the data follow a normal distribution. This is not a flag for automatically excluding data.\nCalculate the Minimum Significant Difference \\(MSD = 2sd\\). This is the minimum difference between two compounds that is statistically significant.\nFor potency data all statistics are transformed back to linear scale and differences become ratios (e.g. Minimum Significant Ratio, \\(MSR = 10^{MSD}\\)).\n\n\n\n\nThe replicate experiment is based on two assumptions:\n\nThe variability in the measurements of all the samples is equivalent.\nThe variability is normally distributed.\n\n\n\n\n\n\n\n\n\nFigure 1: Difference Distribution for Replicate Experiment.\n\n\n\n\n\nFigure 1 illustrates the distribution of difference values along with the associated analysis statistics. The difference limits are derived from the standard error of the mean (SEM) and represent a 95% Confidence Interval (CI) around the mean difference. If the difference limits include 0, then there is no discernible systematic bias between the first and second measurements. Note: As the number of samples increases, a few samples may fall outside the LSA due to random variation, especially if they are just outside of the LSA. If outliers are to be excluded, start with the most extreme differences. Only obvious outliers or those with an assignable cause should be removed.\n\n\nThe two most common types of data reported from in vitro bioassays are efficacy and potency. Understanding the variability of these data types is essential for the interpretation of replicate experiment results.\n\n\nEfficacy measures the magnitude of an effect at a given concentration and is usually what is directly measured in an assay well. The variability in efficacy measurements is normally distributed at a given concentration, though this variability may change across the dynamic range of the assay. There are two primary factors that can contribute to this. The first is the detection method. For example fluorescence intensity often exhibits a constant coefficient of variation (cv) across the dynamic range, while the standard deviation (sd) of ratiometric fluorescence polarization measurements is more constant. Second, the sigmoidal nature of most dose-response curves means that the variability of individual efficacy measurements will be greatest around the inflection point in the curve, due to the effect that small differences in sample concentration can exhibit. While it is difficult to predict the overall effect in advance, the replicate experiment with samples across the full dynamic range of the assay should indicate whether the data can be treated as a single population or should be divided into more discrete populations (e.g. inactive, moderately active, highly active).\n\n\n\nPotency data are the concentration of a substance that elicits a specified efficacy. These are generated by fitting concentration-response curves to the Hill equation (Y vs log(X)) to determine the binding constant and slope. While AC50 values are used for the examples, any potency measurement (Ki, LD80, etc.) can be used.\nPotency values generally follow a log-normal distribution (Elassaiss-Schaap and Duisters 2020). In other words, log transformed potency values are normally distributed. Potency values are determined from the efficacy plotted at log(concentration), which is often represented as an x-axis with a log scale. Therefore when doing a statistical analysis, which assumes the data are normally distributed, the analysis must be done with log transformed potency values.\n\n\n\n\n\n\n\n\nFigure 2: Potency data is log-normal.\n\n\n\n\n\nA simulated data set of 10,000 potency measurements of a compound with a true potency of 100 nM was used to illustrate some of the properties of log-normal data. Figure 2 shows several views of the data distribution. Panel A represents the original data values plotted on a linear axis. Panel B is the log10 transformed potency data and exhibits the expected normal distribution centered on 2 (log10(100) = 2). Panel C is the original data plotted with a log10 transformed axis, which now appears to be normally distributed. Transforming the axis spacing for log-normal data facilitates visualization of the variability in the data. The true potency of 100 is the geometric mean of the data set and should be the median in the distribution of the original data. Note that the peak of the original data is less than the geometric mean, since the data is right skewed. This skew in the linear potency data illustrates why geometric means, rather than arithmetic means, are used for summarizing potency data. The geometric mean corresponds to the center of the log-normal data distribution.\nThis log-normal data distribution means that the data analysis must be performed on log(Potency) data as described in Section 1.4. This ensures that the differences will be normally distributed as shown in Figure 1. These are then converted back to the linear scale for reporting, so differences are transformed to ratios and a difference of 0 becomes a ratio of 1. These transformed values are also log-normal so the axis scales are log transformed for graphing. Note: If potency has already been transformed into a log scale (eg. pKa or log(IC50) it should be analyzed in the Replicate-Experiment web tool with the Efficacy setting.\n\n\n\n\nThe Replicate-Experiment has three primary assumptions. The first assumption it that the differences between each pair of measurements and should be random and normally distributed, Second, the variability is consistent across the range of measured values. Finally, the mean difference for the set of paired measurements should be 0 (or 1 for ratios), if the measured values are equivalent. Bland-Altman plots (Bland and Altman 1986) can be used to examine the assumptions in the replicate experiment. In this plot, the x-axis is the mean of the measurement with the measurement difference on the y-axis.\n\n\n\n\nTable 1: Bland-Altman Data.\n\n\n\n\n\n\n\n\n\nBland-Altman Data\n\n\nSample\nExp1\nExp2\nMean\nDifference\n\n\n\n\n8712555\n-31.693612\n-18.471848\n-25.082730\n-13.221764\n\n\n3168234\n48.123361\n59.983678\n54.053520\n-11.860317\n\n\n6776201\n57.330792\n59.333070\n58.331931\n-2.002279\n\n\n6396802\n52.199878\n43.796948\n47.998413\n8.402931\n\n\n9634108\n48.244736\n55.571406\n51.908071\n-7.326670\n\n\n1196678\n3.678662\n2.016909\n2.847785\n1.661753\n\n\n\n\n\n\n\n\n\n\nA sample data set of 100 samples tested twice can be used to illustrate these concepts. Using just the first 2 steps in Section 1.4 will produce a table with the measurements for each sample along with the mean an difference for each pair as shown in Table 1.\n\n\n\n\n\n\n\n\nFigure 3: Histogram of Difference Values.\n\n\n\n\n\nA histogram of the difference values for each pair of samples is centered around 0 and the distribution is roughly normal Figure 3\n\n\n\n\n\n\n\n\nFigure 4: Bland-Altman Plots.\n\n\n\n\n\nThe summarized data for each pair can then be visualized using Bland Altman plot with the Mean on the x-axis and the Difference on the y-axis, Figure 4 Panel A shows a simulated data set with the y-axis is scaled in units of standard deviation similar to Figure 1 with the same color scheme. Panel B shows the same data bun the y-axis is now scaled to the actual difference values, rather than standard deviations. The x-axis spreads the data across the dynamic range of the measurements, so it’s possible to visualize if the variation is distributed equally across this range. The difference values should cluster near 0 on the y-axis with fewer points as you increase the distance from 0. Panel B shows a more conventional representation. There is a reference line for 0 as well as lines to indicate the mean difference, the difference limits, and the limits of statistical agreement. About half of the data points should be in either side of the mean difference line.\nWhile the standard Bland-Altman plot is appropriate for efficacy data or log transformed potency data, it must be modified to display potency data in the original concentration units. The x-axis is log transformed and the ratio of the two measurements is plotted on the y-axis (see Section 1.5.1.2).\n\n\n\nCorrelation plots directly comparing the measurement values for each sample between the 2 experiments are also useful.\n\n\n\n\n\n\n\n\nFigure 5: Correlation plot.\n\n\n\n\n\nFigure 5 shows the correlation between the two experiments using Spearman’s method. This linear correlation is appropriate for efficacy data and log transformed potency data. To view potency data in the original concentration units, both the x-axis and y-axis should be log transformed to see a linear correlation.\n\n\n\n\nAll the visualizations and tables below were created with the Replicate Experiment web tool, which also adds sample labels to any data outside of the Agreement Limits.\n\n\nEfficacy measurements can be made with raw data from a detector or data which has been normalized (e.g. % Activity) using plate controls. Normalized data is preferred, since it provides biological context and facilitates comparisons across the lifetime of an assay. Note all the example data sets are % activity, where the vehicle control is set to 0.\nGenerally, efficacy experiments can be performed with a single plate of samples tested twice. Even a 96-well plate with control wells will provide enough samples for a good statistical analysis. This data set represents 320 samples tested in 2 independent experiments.\nThis data set represents efficacy data with a constant sd, as described in Section 1.5.1.1.\n\n\n\n\nTable 2: Sample efficacy data (sd constant).\n\n\n\n\n\n\n\n\n\nUploaded Data\n\n\nSample\nExp1\nExp2\n\n\n\n\n7032194\n115.044632\n118.490663\n\n\n2451491\n-7.202141\n1.521088\n\n\n1771761\n35.259537\n44.730147\n\n\n2769859\n20.962247\n23.343282\n\n\n3779070\n106.162994\n108.951022\n\n\n7853477\n-7.261590\n-19.891421\n\n\n\n\n\n\n\n\n\n\nTable 2 shows the data from the first 6 samples. The data is simply the sample identifiers (numeric or character) and the measured activity values for the 2 experiments.\n\n\n\n\nTable 3: Calculated Replicate Experiment Data (sd constant).\n\n\n\n\n\n\n\n\n\nCalculated Data\n\n\nSample\nMeas1\nMeas2\nMeasMean\nMeasDiff\nClass\n\n\n\n\n7032194\n1.15 × 102\n1.18 × 102\n1.17 × 102\n−3.45\nNA\n\n\n2451491\n−7.20\n1.52\n−2.84\n−8.72\nNA\n\n\n1771761\n3.53 × 101\n4.47 × 101\n4.00 × 101\n−9.47\nNA\n\n\n2769859\n2.10 × 101\n2.33 × 101\n2.22 × 101\n−2.38\nNA\n\n\n3779070\n1.06 × 102\n1.09 × 102\n1.08 × 102\n−2.79\nNA\n\n\n7853477\n−7.26\n−1.99 × 101\n−1.36 × 101\n1.26 × 101\nNA\n\n\n\n\n\n\n\n\n\n\nOnce the data have been uploaded, the replicate experiment calculations are displayed, Table 3. This data now includes the Mean and Difference values as well as Class, in addition to the original data. The Class column is used to identify flagged samples that fall outside the agreement limits. This table is fully sortable in the web tool. Note. While the input data may contain any number of digits after the decimal point and are used for the analysis, calculated values and statistics are displayed to 3 significant digits (Dahlin et al. 2019).\n\n\n\n\n\n\nFigure 6: Mean-Difference plot of efficacy data (sd constant).\n\n\n\nThe Bland-Altman plot, Figure 6 shows that the variation is consistent across the range of measured values with most of the difference values close to 0. The statistics in the plot are based on the distribution in Figure 1.\n\n\n\n\nTable 4: Efficacy Replicate Experiment Statistics (sd constant).\n\n\n\n\n\n\n\n\n\nEfficacy Stats\n\n\nn\nMeanDiff\nMSD\nUpper Difference Limit\nLower Difference Limit\nUpper Agreement Limit\nLower Agreement Limit\n\n\n\n\n320\n−1.80 × 10−3\n2.11 × 101\n1.16\n−1.16\n2.08 × 101\n−2.08 × 101\n\n\n\n\n\n\n\n\n\n\nThe statistics for the Bland-Altman plot are shown in Table 4. There are 18 samples with differences outside of the agreement limits (95% CI) which are flagged and labelled in the report graphs. This is close to the expected value of 16 for 320 samples. The MSD is 21.1 and recall that MSD equals 2 standard deviations. It appears that all of the data are within about 3 standard deviations, so none of the flagged samples appear to be obvious outliers.\n\n\n\n\n\n\nFigure 7: Efficacy Correlation plot (sd constant).\n\n\n\nFigure 7 is a correlation plot of the two data sets with a unity reference line and Spearman’s correlation coefficient. The data should overlay the equality reference line.\nThe calculated data, plots, and statistics can be downloaded for documentation and future reference.\n\n\n\nOften the variability in efficacy data varies across the dynamic range of an assay and is better represented by the CV, Section 1.5.1.1. Unfortunately this violates a primary assumption in the analysis, making the replicate-experiment less useful. This is illustrated in the following example, where the CV is 10% across the assay range.\n\n\n\n\n\n\nFigure 8: Mean Difference plot for an assay with 10% CV.\n\n\n\nFigure 8 shows the variability increasing as the efficacy values increase. This occurs in signal increase assays, where the raw data values for the vehicle controls are smaller than those for active samples. The pattern would be reversed for a signal decrease assay.\n\n\n\n\nTable 5: Efficacy (cv constant) Replicate Experiment Statistics.\n\n\n\n\n\n\n\n\n\nEfficacy Stats (Constant cv\n\n\nn\nMeanDiff\nMSD\nUpper Difference Limit\nLower Difference Limit\nUpper Agreement Limit\nLower Agreement Limit\n\n\n\n\n320\n−1.05 × 10−1\n1.82 × 101\n8.96 × 10−1\n−1.11\n1.78 × 101\n−1.80 × 101\n\n\n\n\n\n\n\n\n\n\nTable 5 shows the statistics associated with Figure 8. Since the variability in the measurements is not constant across the dynamic range of the assay, it violates one of the assumptions of the Bland-Altman analysis, Section 1.5.2. Therefore these statistics are not meaningful.\n\n\n\n\n\n\nFigure 9: Correlation plot for an assay with 10% CV.\n\n\n\nThe same pattern is observed in the correlation plot, Figure 9, with the spread between experiments increasing as the efficacy increases.\nAssays which show this behavior should not use the Replicate-Experiment as a general measurement of uncertainty in the assay. However Replicate-Experiments with the vehicle and active controls analyzed separately can be useful to document that the assay is repeatable and help to define the limits for activity.\n\n\n\nBelow are the first 6 samples from an example experiment with 32 total samples. The data is simply the sample identifiers (numeric or character) and the measured potency values for the 2 experiments. This is all that is needed to upload for analysis.\n\n\n\n\nTable 6: Sample Potency data.\n\n\n\n\n\n\n\n\n\nUploaded Data\n\n\nSample\nExp1\nExp2\n\n\n\n\n2455167\n883.59032115\n531.29130588\n\n\n2225318\n0.33218715\n1.03798254\n\n\n4625166\n2.16832037\n2.15084190\n\n\n9852359\n2.55900969\n2.06438777\n\n\n2107695\n146.93978930\n214.71751251\n\n\n7465843\n0.07546636\n0.08069902\n\n\n\n\n\n\n\n\n\n\nThe data analysis is similar to that used with efficacy data, except that the potency values must first be transformed to their log10 values, so the variability will have a normal distribution for the statistical analysis. Then the means and differences for the pairs of log10(Potency) values are determined along with the associated statistics as described in Section 1.4.\n\n\n\n\nTable 7: Calculated Replicate Experiment Data.\n\n\n\n\n\n\n\n\n\nCalculated Data\n\n\nSample\nExp1\nExp2\nGeometric Mean\nRatio\n\n\n\n\n2455167\n8.84 × 102\n5.31 × 102\n6.85 × 102\n1.66\n\n\n2225318\n3.32 × 10−1\n1.04\n5.87 × 10−1\n3.20 × 10−1\n\n\n4625166\n2.17\n2.15\n2.16\n1.01\n\n\n9852359\n2.56\n2.06\n2.30\n1.24\n\n\n2107695\n1.47 × 102\n2.15 × 102\n1.78 × 102\n6.84 × 10−1\n\n\n7465843\n7.55 × 10−2\n8.07 × 10−2\n7.80 × 10−2\n9.35 × 10−1\n\n\n\n\n\n\n\n\n\n\nOnce the data have been uploaded, the Replicate-Experiment calculations are displayed, as shown in Table 7. This data now includes the Geometric Mean and Ratio values as well as Class, in addition to the original data. The Geometric Mean and the Ratio represent the mean and difference values for the log(Potency) data after they have been transformed back to the original linear scale. Recall that \\(log(A) - log(B) = log(A/B)\\), so the difference between two log values becomes a ratio, when anti-logged. Similarly, the mean of logs becomes a geometric mean when anti-logged. The Class column is used to identify flagged samples that fall outside the agreement limits. This table is fully sortable in the web tool. Note. While the input data may contain any number of digits after the decimal point and are used for the analysis. Calculated values and statistics are displayed to 3 significant digits (Dahlin et al. 2019).\n\n\n\n\n\n\nFigure 10: Mean-Ratio plot of potency data.\n\n\n\nThe Bland-Altman plot, Figure 10, for potency data uses the geometric mean value of each data pair on the x-axis with the ratio between the potencies is represented on the y-axis. Both axes are plotted on the log scale, as described in Section 1.5.1.2. The data points are centered around 1 with about half of the data on either side of the center. The variability is also evenly distributed across the range of x values.\n\n\n\n\nTable 8: Potency Replicate Experiment Statistics.\n\n\n\n\n\n\n\n\n\nPotency Stats\n\n\nn\nMeanRatio\nMSR\nUpper Ratio Limit\nLower Ratio Limit\nUpper Agreement Limit\nLower Agreement Limit\n\n\n\n\n32\n1.06\n2.53\n1.25\n8.98 × 10−1\n2.73\n4.13 × 10−1\n\n\n\n\n\n\n\n\n\n\nThe corresponding statistics are shown in Table 8. There is only a single value labelled as an outlier in the Bland-Altman plot. This is consistent with a data set of this size. The ratio for this sample, while outside of the ratio limits, is close and thus not an obvious outlier.\n\n\n\n\n\n\nFigure 11: Potency Correlation plot.\n\n\n\nFigure 11 is a correlation plot of the two data sets with a unity reference line and the Spearman correlation. The data should overlay the equality reference line.\nThe calculated data, plots, and statistics can be downloaded for documentation and future reference.\n\n\nA Replicate-Experiment MSR \\(\\leq 3.0\\) and limits of agreement between 1/3 and 3 is generally acceptable for most assays. An MSR \\(\\leq 5.0\\) may be acceptable in a secondary assay, where the data will be used for more categorical decisions (e.g. &gt; 100-fold selectivity vs. the primary assay). If more precision is needed, that can be achieved by increasing the number of times that each sample is independently tested. The MSR calculation for the Replicate-Experiment assumes that most samples tested will only be measured a single time. MSR is dependent upon the number of routine replicates (Haas et al. 2017): \\[MSR = 10^{2\\sqrt{2(s/\\sqrt{n})}}\\]\n\n\n\n\nTable 9: MSR values assuming different numbers of independent replicates.\n\n\n\n\n\n\n\n\n\nn = 1\nn = 2\nn = 3\nn = 4\nn = 5\nn = 6\nn = 7\nn = 8\n\n\n\n\n2.53\n1.93\n1.71\n1.59\n1.51\n1.46\n1.42\n1.39\n\n\n\n\n\n\n\n\n\n\nTable 9 shows the calculated MSR for different numbers of independent replicates. The value of n indicates the number of different experiments each compound should be tested in to reach the calculated MSR.\n\n\n\n\nEarly in a project, it may be a struggle to identify a sufficient number of validated, active samples to perform a replicate experiment. Here is an example where there are only 5 unique samples. Unfortunately, this low sample number does not provide enough statistical power to obtain a good estimate of the assay’s variability, so the Ratio Limits and MSR can appear to be greater than their true values.\n\n\n\n\n\n\nFigure 12: Mean Ratio plot.\n\n\n\n\n\n\n\nTable 10: Replicate Experiment Statistics.\n\n\n\n\n\n\n\n\n\nPotency Stats\n\n\nn\nMeanRatio\nMSR\nUpper Ratio Limit\nLower Ratio Limit\nUpper Agreement Limit\nLower Agreement Limit\n\n\n\n\n5\n1.16\n1.01 × 101\n4.88\n2.75 × 10−1\n2.89 × 101\n4.64 × 10−2\n\n\n\n\n\n\n\n\n\n\nThe Bland-Altman plot, Figure 12, and the associated statistics, Table 10, illustrate the problem. While the MSR appears to be high, we also see that n is only 5, so there is probably not enough data to provide a good estimate of the MSR.\n\n\n\n\n\n\nFigure 13: Correlation Plot\n\n\n\nFortunately, there is a simple solution to this problem. Each sample can be replicated multiple times on the plates. The replicate dose-response curves are then analyzed as if they were from different samples. Ideally, these replicate samples should be prepared independently, as if they were unique compounds with their own dilution series. In this example, the 5 unique samples were each replicated 6 times (e.g. sample 1 becomes 1A, 1B, 1C …). This produces 30 pairs of potency values for the analysis\n\n\n\n\n\n\nFigure 14: Replicated samples Mean Ratio plot.\n\n\n\nThe Bland-Altman plot, Figure 14, now shows 5 clusters of difference values corresponding to the 5 replicated samples.\n\n\n\n\nTable 11: Replicate Experiment Statistics.\n\n\n\n\n\n\n\n\n\nPotency Stats\n\n\nn\nMeanRatio\nMSR\nUpper Ratio Limit\nLower Ratio Limit\nUpper Agreement Limit\nLower Agreement Limit\n\n\n\n\n30\n9.90 × 10−1\n3.53\n1.25\n7.83 × 10−1\n3.59\n2.73 × 10−1\n\n\n\n\n\n\n\n\n\n\nThe associated statistics, Table 11, show a much lower MSR.\n\n\n\n\n\n\nFigure 15: Replicated samples correlation plot.\n\n\n\nSimilar clustering is observed in the correlation plot, Figure 15. While it is easy to identify the clusters due to the sample replication in the graphs, the improvement in statistical power from n=30 instead of n = 5 provides a much better estimate of the true assay variability. Both the MSR and the ratio limits are significantly lower.\n\n\n\nSystematic differences between two experiments could be indicated when the mean ratio line is displaced from the ratio = 1 reference line. If the reference line is outside of the Ratio Limits for the Center Line, the difference is statistically significant and should be investigated to determine the cause. However, if the reference line is within the Ratio Limits, it could simply be random variation or to a cause with a small effect.\n\n\n\n\nTable 12: Replicate Experiment Potency Shift Data.\n\n\n\n\n\n\n\n\n\nPotency Shift Data.\n\n\nSample\nExp1\nExp2\nGeometric_Mean\nRatio\n\n\n\n\n4649104\n2.45\n5.28\n3.60\n4.64 × 10−1\n\n\n2303569\n1.68 × 103\n2.33 × 103\n1.98 × 103\n7.20 × 10−1\n\n\n2002357\n1.01 × 102\n1.05 × 102\n1.03 × 102\n9.62 × 10−1\n\n\n8605428\n1.25 × 101\n5.99 × 101\n2.73 × 101\n2.08 × 10−1\n\n\n6472482\n1.13 × 102\n2.03 × 102\n1.51 × 102\n5.56 × 10−1\n\n\n1210635\n2.04 × 10−3\n3.97 × 10−3\n2.85 × 10−3\n5.13 × 10−1\n\n\n\n\n\n\n\n\n\n\nThe data in Table 12 is used to illustrate this situation.\n\n\n\n\n\n\nFigure 16: Systematic error Bland-Altman plot.\n\n\n\nNow the black reference line (Ratio = 1) is clearly outside of the observed Ratio Limits around the blue Center Line for the data. This indicates that there is a systematic shift in the data between the first and second measurements of the compound potencies.\n\n\n\n\nTable 13: Potency Shift Replicate Experiment Statistics.\n\n\n\n\n\n\n\n\n\nPotency Stats\n\n\nn\nMeanRatio\nMSR\nUpper Ratio Limit\nLower Ratio Limit\nUpper Agreement Limit\nLower Agreement Limit\n\n\n\n\n32\n1.06\n2.53\n1.25\n8.98 × 10−1\n2.73\n4.13 × 10−1\n\n\n\n\n\n\n\n\n\n\nThe associated statistics, Table 13, also shows that the expected ratio of 1 is not within the ratio limits.\n\n\n\n\n\n\nFigure 17: Systematic Shift Correlation Plot.\n\n\n\nThe correlation plot, Figure 17, also shows separation between the reference identity line and the correlation line.\nCommon causes can include any of the following:\n\nDifferences in the samples between experiments (e.g. sample lots, or solubilizations).\nReagents or cells used in the assay.\nEquipment changes.\nEnvironmental changes (e.g. temperature).\nPersonnel\n\nIf a cause is identified, it should be mitigated (if possible) and the replicate experiment repeated. Sometimes a cause may be identified which includes historical data that can’t be changed. If this happens, it may be significant enough to require retesting of key compounds to assess the impact.\n\n\n\n\nThe Assay Guidance Manual Webtools are supported by the Intramural Research Program of the National Center for Advancing Translational Sciences at the National Institutes of Health (ZIA TR000340)."
  },
  {
    "objectID": "RepExpDoc.html#introduction",
    "href": "RepExpDoc.html#introduction",
    "title": "Replicate Experiment",
    "section": "",
    "text": "Statistical process control (SPC) is the use of statistical methods to monitor and control the quality of processes. Though the tools were initially developed and used for manufacturing, they have found applications in many other areas where outcomes can be measured and quality is valued, including scientific laboratories. Some of the most common SPC tools include Run Charts, Control Charts, Experimental Design, and Replicate Experiment.\nThe Replicate-Experiment (Eastwood et al. 2006a) is based on Bland-Altman difference analysis (Bland and Altman 1986) between two sets of measurements and is used to assess either repeatability or reproducibility. These terms have specific meanings in SPC. Repeatability is the ability to produce comparable results within a group of conditions (e.g. people, instruments, reagents, …), while reproducibility refers to the ability to replicate results across different conditions (e.g. between two assayers or pieces of equipment). Initially repeatability should be demonstrated by having a scientist test the same set of samples with 2 independently prepared sets of working reagents/cells (note: these experiments can be performed at the same time or on different days) on the same set of equipment. Once repeatability has been established, reproducibility can be determined between different assayers, pieces of equipment, or lots of reagents or cells."
  },
  {
    "objectID": "RepExpDoc.html#rationale",
    "href": "RepExpDoc.html#rationale",
    "title": "Replicate Experiment",
    "section": "",
    "text": "Replicate-Experiment studies (Iversen et al. 2012) are used to formally evaluate the within-run assay variability and formally compare a new assay to the existing (old) assay. They also allow a preliminary assessment of the overall or between-run assay variability, but two runs are not enough to adequately assess overall variability. Post-production monitoring, such as Retrospective MSR (Haas et al. 2017) analysis and Control Charts (Beck et al. 2017) are used to formally evaluate the overall variability in the assay. Note that the Replicate-Experiment study is a diagnostic and decision tool used to establish that the assay is ready to go into production by showing that the endpoints of the assay are repeatable over a range of values. It is not intended as a substitute for post-production monitoring or to provide an estimate of the overall Minimum Significant Ratio (MSR).\nIt may seem counter-intuitive to call the differences between two independent assay runs as within-run variability. However, the terminology results from how assay runs are defined. Experimental variation is categorized into two distinct components: between-run and within-run sources. Consider the following examples:\n\nIf there is variation in the concentrations of buffer components between 2 runs, then the assay results could be affected. However, assuming that the same buffer is used with all compounds within one run, each compound will be equally affected and so the difference will only show up when comparing one run to another run, i.e. in two runs, one run will appear higher on average than the other run. This variation is called between-run variation.\nIf the concentration of a compound in the stock plate varies from the target concentration then all wells where that compound is used will be affected. However, wells used to test other compounds will be unaffected. This type of variation is called within-run as the source of variation affects different compounds in the same run differently.\nSome sources of variability affect both within- and between-run variation. For example, if assay cells are plated and then incubated for 24-72 hours to achieve a target cell density taking into account the doubling time of the cells. If the doubling time equals the incubation time, and the target density is 30,000 cells/well, then 15,000 cells/well are plated. But even if exactly 15,000 cells are placed in each well there won’t be exactly 30,000 cells in each well after 24 hours. Some will be lower and some will be higher than the target. These differences are within-run as not all wells are equally affected. But also suppose in a particular run only 13,000 cells are initially plated. Then the wells will on average have fewer than 30,000 cells after 24 hours, and since all cells are affected this is between-run variation. Thus cell density has both within- and between-run sources of variation.\n\nThe total variation is the sum of both sources of variation. When comparing two compounds across runs, one must take into account both the within-run and between-run sources of variation. But when comparing two compounds in the same run, one must only take into account the within-run sources, since, by definition, the between-run sources affect both compounds equally.\nIn a Replicate-Experiment study, the between-run sources of variation cause one run to be on average higher than the other run. However, it would be very unlikely that the differences between the two runs were exactly the same for every compound in the study. These individual compound “differences from the average difference” are caused by the within-run sources of variation. The higher the within-run variability the greater the individual compound variation in the assay runs.\nThe analysis approach used in the Replicate-Experiment study is to estimate and factor out between-run variability, and then estimate the magnitude of within-run variability."
  },
  {
    "objectID": "RepExpDoc.html#experimental-procedure",
    "href": "RepExpDoc.html#experimental-procedure",
    "title": "Replicate Experiment",
    "section": "",
    "text": "The Replicate-Experiment is intended to be easy to execute with a modest resource commitment. Most executions can be performed with 2-4 assay plates. It is most commonly run in the potency mode, though it can be run in efficacy mode to gain a better understanding of assay variability across the dynamic range of the assay or facilitate the interpretation of screening results.\nThe potency mode is ideally run with 20-30 active compounds with a broad range of potencies and the potencies should be well spaced across the range of values. If this number of active compounds is not available, then it can be run with a smaller set of compounds in replicate, with each replicate treated as an independent sample (e.g. 5 compounds with 5 separate replicate dilutions).\nIn the efficacy mode, it is particularly important to have samples where the activity spans the dynamic range of the assay. Often the variability of measurements will not be constant across the dynamic range of the assay. For this reason, efficacy studies should not be conducted with random screening plates, since most compounds will be inactive and could skew the assessment. It may be simpler to use a small number of active compounds in a dilution series, as if for a potency determination, but treat each dilution as an independent sample for the efficacy analysis. This will ensure that the data cover the entire dynamic range of the assay with just a few compounds.\nInitially repeatability should be demonstrated with identical compounds tested with 2 independently prepared sets of reagents/cells. Once repeatability has been demonstrated for a protocol, it is ready for routine testing. The assay should be monitored to ensure it behaves as validated. This can include any or all of the following methods:\n\nControl charting reference compound(s) (Beck et al. 2017).\nRetrospective MSR analysis (Haas et al. 2017).\nperiodic retests if the samples used in previous replicate experiments to compare to previous data (Eastwood et al. 2006b).\n\nThe replicate experiment is also used to validate minor assay changes such as new assayers, equipment substitutions, or changes to lots of reagents or cells. In this case, the data from identical samples is compared in the current and new formats. Historical data can be used for the current format, as long as it comes from a single experiment."
  },
  {
    "objectID": "RepExpDoc.html#sec-data_analysis",
    "href": "RepExpDoc.html#sec-data_analysis",
    "title": "Replicate Experiment",
    "section": "",
    "text": "The statistical analysis assumes that any measurement errors are normally distributed. While this is true for efficacy data, potency data is log-normal. This means that potency data must first be transformed to their log10 values before the analysis. After that transformation the analysis methods are the same:\n\nFor each pair of compound measurements, calculate the mean = (meas1 + meas2)/2 and the difference = (meas1 - meas2). Note when these values are transformed back to the linear scale for potency data they will generate the geometric mean and the ratio, since log(meas1) - log(meas2) = log(meas1/meas2).\nCalculate the mean (\\(\\bar{d}\\)) and standard deviation (sd) for the set of the difference values.\nCalculate the difference limits \\(DLs = \\bar{d}\\pm2sd/\\sqrt{n}\\) where n is the number of compounds tested. This is the 95% confidence interval for the mean difference.\nCalculate limits of agreement \\(LSAs = \\bar{d}\\pm2sd\\). Most of the individual compound differences (~95%) should fall within these limits.\nSamples outside the agreement limits are flagged as outliers. Since these values are 2sd outliers, it is expected that 5% of the data to be flagged if the data follow a normal distribution. This is not a flag for automatically excluding data.\nCalculate the Minimum Significant Difference \\(MSD = 2sd\\). This is the minimum difference between two compounds that is statistically significant.\nFor potency data all statistics are transformed back to linear scale and differences become ratios (e.g. Minimum Significant Ratio, \\(MSR = 10^{MSD}\\))."
  },
  {
    "objectID": "RepExpDoc.html#interpretation",
    "href": "RepExpDoc.html#interpretation",
    "title": "Replicate Experiment",
    "section": "",
    "text": "The replicate experiment is based on two assumptions:\n\nThe variability in the measurements of all the samples is equivalent.\nThe variability is normally distributed.\n\n\n\n\n\n\n\n\n\nFigure 1: Difference Distribution for Replicate Experiment.\n\n\n\n\n\nFigure 1 illustrates the distribution of difference values along with the associated analysis statistics. The difference limits are derived from the standard error of the mean (SEM) and represent a 95% Confidence Interval (CI) around the mean difference. If the difference limits include 0, then there is no discernible systematic bias between the first and second measurements. Note: As the number of samples increases, a few samples may fall outside the LSA due to random variation, especially if they are just outside of the LSA. If outliers are to be excluded, start with the most extreme differences. Only obvious outliers or those with an assignable cause should be removed.\n\n\nThe two most common types of data reported from in vitro bioassays are efficacy and potency. Understanding the variability of these data types is essential for the interpretation of replicate experiment results.\n\n\nEfficacy measures the magnitude of an effect at a given concentration and is usually what is directly measured in an assay well. The variability in efficacy measurements is normally distributed at a given concentration, though this variability may change across the dynamic range of the assay. There are two primary factors that can contribute to this. The first is the detection method. For example fluorescence intensity often exhibits a constant coefficient of variation (cv) across the dynamic range, while the standard deviation (sd) of ratiometric fluorescence polarization measurements is more constant. Second, the sigmoidal nature of most dose-response curves means that the variability of individual efficacy measurements will be greatest around the inflection point in the curve, due to the effect that small differences in sample concentration can exhibit. While it is difficult to predict the overall effect in advance, the replicate experiment with samples across the full dynamic range of the assay should indicate whether the data can be treated as a single population or should be divided into more discrete populations (e.g. inactive, moderately active, highly active).\n\n\n\nPotency data are the concentration of a substance that elicits a specified efficacy. These are generated by fitting concentration-response curves to the Hill equation (Y vs log(X)) to determine the binding constant and slope. While AC50 values are used for the examples, any potency measurement (Ki, LD80, etc.) can be used.\nPotency values generally follow a log-normal distribution (Elassaiss-Schaap and Duisters 2020). In other words, log transformed potency values are normally distributed. Potency values are determined from the efficacy plotted at log(concentration), which is often represented as an x-axis with a log scale. Therefore when doing a statistical analysis, which assumes the data are normally distributed, the analysis must be done with log transformed potency values.\n\n\n\n\n\n\n\n\nFigure 2: Potency data is log-normal.\n\n\n\n\n\nA simulated data set of 10,000 potency measurements of a compound with a true potency of 100 nM was used to illustrate some of the properties of log-normal data. Figure 2 shows several views of the data distribution. Panel A represents the original data values plotted on a linear axis. Panel B is the log10 transformed potency data and exhibits the expected normal distribution centered on 2 (log10(100) = 2). Panel C is the original data plotted with a log10 transformed axis, which now appears to be normally distributed. Transforming the axis spacing for log-normal data facilitates visualization of the variability in the data. The true potency of 100 is the geometric mean of the data set and should be the median in the distribution of the original data. Note that the peak of the original data is less than the geometric mean, since the data is right skewed. This skew in the linear potency data illustrates why geometric means, rather than arithmetic means, are used for summarizing potency data. The geometric mean corresponds to the center of the log-normal data distribution.\nThis log-normal data distribution means that the data analysis must be performed on log(Potency) data as described in Section 1.4. This ensures that the differences will be normally distributed as shown in Figure 1. These are then converted back to the linear scale for reporting, so differences are transformed to ratios and a difference of 0 becomes a ratio of 1. These transformed values are also log-normal so the axis scales are log transformed for graphing. Note: If potency has already been transformed into a log scale (eg. pKa or log(IC50) it should be analyzed in the Replicate-Experiment web tool with the Efficacy setting.\n\n\n\n\nThe Replicate-Experiment has three primary assumptions. The first assumption it that the differences between each pair of measurements and should be random and normally distributed, Second, the variability is consistent across the range of measured values. Finally, the mean difference for the set of paired measurements should be 0 (or 1 for ratios), if the measured values are equivalent. Bland-Altman plots (Bland and Altman 1986) can be used to examine the assumptions in the replicate experiment. In this plot, the x-axis is the mean of the measurement with the measurement difference on the y-axis.\n\n\n\n\nTable 1: Bland-Altman Data.\n\n\n\n\n\n\n\n\n\nBland-Altman Data\n\n\nSample\nExp1\nExp2\nMean\nDifference\n\n\n\n\n8712555\n-31.693612\n-18.471848\n-25.082730\n-13.221764\n\n\n3168234\n48.123361\n59.983678\n54.053520\n-11.860317\n\n\n6776201\n57.330792\n59.333070\n58.331931\n-2.002279\n\n\n6396802\n52.199878\n43.796948\n47.998413\n8.402931\n\n\n9634108\n48.244736\n55.571406\n51.908071\n-7.326670\n\n\n1196678\n3.678662\n2.016909\n2.847785\n1.661753\n\n\n\n\n\n\n\n\n\n\nA sample data set of 100 samples tested twice can be used to illustrate these concepts. Using just the first 2 steps in Section 1.4 will produce a table with the measurements for each sample along with the mean an difference for each pair as shown in Table 1.\n\n\n\n\n\n\n\n\nFigure 3: Histogram of Difference Values.\n\n\n\n\n\nA histogram of the difference values for each pair of samples is centered around 0 and the distribution is roughly normal Figure 3\n\n\n\n\n\n\n\n\nFigure 4: Bland-Altman Plots.\n\n\n\n\n\nThe summarized data for each pair can then be visualized using Bland Altman plot with the Mean on the x-axis and the Difference on the y-axis, Figure 4 Panel A shows a simulated data set with the y-axis is scaled in units of standard deviation similar to Figure 1 with the same color scheme. Panel B shows the same data bun the y-axis is now scaled to the actual difference values, rather than standard deviations. The x-axis spreads the data across the dynamic range of the measurements, so it’s possible to visualize if the variation is distributed equally across this range. The difference values should cluster near 0 on the y-axis with fewer points as you increase the distance from 0. Panel B shows a more conventional representation. There is a reference line for 0 as well as lines to indicate the mean difference, the difference limits, and the limits of statistical agreement. About half of the data points should be in either side of the mean difference line.\nWhile the standard Bland-Altman plot is appropriate for efficacy data or log transformed potency data, it must be modified to display potency data in the original concentration units. The x-axis is log transformed and the ratio of the two measurements is plotted on the y-axis (see Section 1.5.1.2).\n\n\n\nCorrelation plots directly comparing the measurement values for each sample between the 2 experiments are also useful.\n\n\n\n\n\n\n\n\nFigure 5: Correlation plot.\n\n\n\n\n\nFigure 5 shows the correlation between the two experiments using Spearman’s method. This linear correlation is appropriate for efficacy data and log transformed potency data. To view potency data in the original concentration units, both the x-axis and y-axis should be log transformed to see a linear correlation."
  },
  {
    "objectID": "RepExpDoc.html#examples",
    "href": "RepExpDoc.html#examples",
    "title": "Replicate Experiment",
    "section": "",
    "text": "All the visualizations and tables below were created with the Replicate Experiment web tool, which also adds sample labels to any data outside of the Agreement Limits.\n\n\nEfficacy measurements can be made with raw data from a detector or data which has been normalized (e.g. % Activity) using plate controls. Normalized data is preferred, since it provides biological context and facilitates comparisons across the lifetime of an assay. Note all the example data sets are % activity, where the vehicle control is set to 0.\nGenerally, efficacy experiments can be performed with a single plate of samples tested twice. Even a 96-well plate with control wells will provide enough samples for a good statistical analysis. This data set represents 320 samples tested in 2 independent experiments.\nThis data set represents efficacy data with a constant sd, as described in Section 1.5.1.1.\n\n\n\n\nTable 2: Sample efficacy data (sd constant).\n\n\n\n\n\n\n\n\n\nUploaded Data\n\n\nSample\nExp1\nExp2\n\n\n\n\n7032194\n115.044632\n118.490663\n\n\n2451491\n-7.202141\n1.521088\n\n\n1771761\n35.259537\n44.730147\n\n\n2769859\n20.962247\n23.343282\n\n\n3779070\n106.162994\n108.951022\n\n\n7853477\n-7.261590\n-19.891421\n\n\n\n\n\n\n\n\n\n\nTable 2 shows the data from the first 6 samples. The data is simply the sample identifiers (numeric or character) and the measured activity values for the 2 experiments.\n\n\n\n\nTable 3: Calculated Replicate Experiment Data (sd constant).\n\n\n\n\n\n\n\n\n\nCalculated Data\n\n\nSample\nMeas1\nMeas2\nMeasMean\nMeasDiff\nClass\n\n\n\n\n7032194\n1.15 × 102\n1.18 × 102\n1.17 × 102\n−3.45\nNA\n\n\n2451491\n−7.20\n1.52\n−2.84\n−8.72\nNA\n\n\n1771761\n3.53 × 101\n4.47 × 101\n4.00 × 101\n−9.47\nNA\n\n\n2769859\n2.10 × 101\n2.33 × 101\n2.22 × 101\n−2.38\nNA\n\n\n3779070\n1.06 × 102\n1.09 × 102\n1.08 × 102\n−2.79\nNA\n\n\n7853477\n−7.26\n−1.99 × 101\n−1.36 × 101\n1.26 × 101\nNA\n\n\n\n\n\n\n\n\n\n\nOnce the data have been uploaded, the replicate experiment calculations are displayed, Table 3. This data now includes the Mean and Difference values as well as Class, in addition to the original data. The Class column is used to identify flagged samples that fall outside the agreement limits. This table is fully sortable in the web tool. Note. While the input data may contain any number of digits after the decimal point and are used for the analysis, calculated values and statistics are displayed to 3 significant digits (Dahlin et al. 2019).\n\n\n\n\n\n\nFigure 6: Mean-Difference plot of efficacy data (sd constant).\n\n\n\nThe Bland-Altman plot, Figure 6 shows that the variation is consistent across the range of measured values with most of the difference values close to 0. The statistics in the plot are based on the distribution in Figure 1.\n\n\n\n\nTable 4: Efficacy Replicate Experiment Statistics (sd constant).\n\n\n\n\n\n\n\n\n\nEfficacy Stats\n\n\nn\nMeanDiff\nMSD\nUpper Difference Limit\nLower Difference Limit\nUpper Agreement Limit\nLower Agreement Limit\n\n\n\n\n320\n−1.80 × 10−3\n2.11 × 101\n1.16\n−1.16\n2.08 × 101\n−2.08 × 101\n\n\n\n\n\n\n\n\n\n\nThe statistics for the Bland-Altman plot are shown in Table 4. There are 18 samples with differences outside of the agreement limits (95% CI) which are flagged and labelled in the report graphs. This is close to the expected value of 16 for 320 samples. The MSD is 21.1 and recall that MSD equals 2 standard deviations. It appears that all of the data are within about 3 standard deviations, so none of the flagged samples appear to be obvious outliers.\n\n\n\n\n\n\nFigure 7: Efficacy Correlation plot (sd constant).\n\n\n\nFigure 7 is a correlation plot of the two data sets with a unity reference line and Spearman’s correlation coefficient. The data should overlay the equality reference line.\nThe calculated data, plots, and statistics can be downloaded for documentation and future reference.\n\n\n\nOften the variability in efficacy data varies across the dynamic range of an assay and is better represented by the CV, Section 1.5.1.1. Unfortunately this violates a primary assumption in the analysis, making the replicate-experiment less useful. This is illustrated in the following example, where the CV is 10% across the assay range.\n\n\n\n\n\n\nFigure 8: Mean Difference plot for an assay with 10% CV.\n\n\n\nFigure 8 shows the variability increasing as the efficacy values increase. This occurs in signal increase assays, where the raw data values for the vehicle controls are smaller than those for active samples. The pattern would be reversed for a signal decrease assay.\n\n\n\n\nTable 5: Efficacy (cv constant) Replicate Experiment Statistics.\n\n\n\n\n\n\n\n\n\nEfficacy Stats (Constant cv\n\n\nn\nMeanDiff\nMSD\nUpper Difference Limit\nLower Difference Limit\nUpper Agreement Limit\nLower Agreement Limit\n\n\n\n\n320\n−1.05 × 10−1\n1.82 × 101\n8.96 × 10−1\n−1.11\n1.78 × 101\n−1.80 × 101\n\n\n\n\n\n\n\n\n\n\nTable 5 shows the statistics associated with Figure 8. Since the variability in the measurements is not constant across the dynamic range of the assay, it violates one of the assumptions of the Bland-Altman analysis, Section 1.5.2. Therefore these statistics are not meaningful.\n\n\n\n\n\n\nFigure 9: Correlation plot for an assay with 10% CV.\n\n\n\nThe same pattern is observed in the correlation plot, Figure 9, with the spread between experiments increasing as the efficacy increases.\nAssays which show this behavior should not use the Replicate-Experiment as a general measurement of uncertainty in the assay. However Replicate-Experiments with the vehicle and active controls analyzed separately can be useful to document that the assay is repeatable and help to define the limits for activity.\n\n\n\nBelow are the first 6 samples from an example experiment with 32 total samples. The data is simply the sample identifiers (numeric or character) and the measured potency values for the 2 experiments. This is all that is needed to upload for analysis.\n\n\n\n\nTable 6: Sample Potency data.\n\n\n\n\n\n\n\n\n\nUploaded Data\n\n\nSample\nExp1\nExp2\n\n\n\n\n2455167\n883.59032115\n531.29130588\n\n\n2225318\n0.33218715\n1.03798254\n\n\n4625166\n2.16832037\n2.15084190\n\n\n9852359\n2.55900969\n2.06438777\n\n\n2107695\n146.93978930\n214.71751251\n\n\n7465843\n0.07546636\n0.08069902\n\n\n\n\n\n\n\n\n\n\nThe data analysis is similar to that used with efficacy data, except that the potency values must first be transformed to their log10 values, so the variability will have a normal distribution for the statistical analysis. Then the means and differences for the pairs of log10(Potency) values are determined along with the associated statistics as described in Section 1.4.\n\n\n\n\nTable 7: Calculated Replicate Experiment Data.\n\n\n\n\n\n\n\n\n\nCalculated Data\n\n\nSample\nExp1\nExp2\nGeometric Mean\nRatio\n\n\n\n\n2455167\n8.84 × 102\n5.31 × 102\n6.85 × 102\n1.66\n\n\n2225318\n3.32 × 10−1\n1.04\n5.87 × 10−1\n3.20 × 10−1\n\n\n4625166\n2.17\n2.15\n2.16\n1.01\n\n\n9852359\n2.56\n2.06\n2.30\n1.24\n\n\n2107695\n1.47 × 102\n2.15 × 102\n1.78 × 102\n6.84 × 10−1\n\n\n7465843\n7.55 × 10−2\n8.07 × 10−2\n7.80 × 10−2\n9.35 × 10−1\n\n\n\n\n\n\n\n\n\n\nOnce the data have been uploaded, the Replicate-Experiment calculations are displayed, as shown in Table 7. This data now includes the Geometric Mean and Ratio values as well as Class, in addition to the original data. The Geometric Mean and the Ratio represent the mean and difference values for the log(Potency) data after they have been transformed back to the original linear scale. Recall that \\(log(A) - log(B) = log(A/B)\\), so the difference between two log values becomes a ratio, when anti-logged. Similarly, the mean of logs becomes a geometric mean when anti-logged. The Class column is used to identify flagged samples that fall outside the agreement limits. This table is fully sortable in the web tool. Note. While the input data may contain any number of digits after the decimal point and are used for the analysis. Calculated values and statistics are displayed to 3 significant digits (Dahlin et al. 2019).\n\n\n\n\n\n\nFigure 10: Mean-Ratio plot of potency data.\n\n\n\nThe Bland-Altman plot, Figure 10, for potency data uses the geometric mean value of each data pair on the x-axis with the ratio between the potencies is represented on the y-axis. Both axes are plotted on the log scale, as described in Section 1.5.1.2. The data points are centered around 1 with about half of the data on either side of the center. The variability is also evenly distributed across the range of x values.\n\n\n\n\nTable 8: Potency Replicate Experiment Statistics.\n\n\n\n\n\n\n\n\n\nPotency Stats\n\n\nn\nMeanRatio\nMSR\nUpper Ratio Limit\nLower Ratio Limit\nUpper Agreement Limit\nLower Agreement Limit\n\n\n\n\n32\n1.06\n2.53\n1.25\n8.98 × 10−1\n2.73\n4.13 × 10−1\n\n\n\n\n\n\n\n\n\n\nThe corresponding statistics are shown in Table 8. There is only a single value labelled as an outlier in the Bland-Altman plot. This is consistent with a data set of this size. The ratio for this sample, while outside of the ratio limits, is close and thus not an obvious outlier.\n\n\n\n\n\n\nFigure 11: Potency Correlation plot.\n\n\n\nFigure 11 is a correlation plot of the two data sets with a unity reference line and the Spearman correlation. The data should overlay the equality reference line.\nThe calculated data, plots, and statistics can be downloaded for documentation and future reference.\n\n\nA Replicate-Experiment MSR \\(\\leq 3.0\\) and limits of agreement between 1/3 and 3 is generally acceptable for most assays. An MSR \\(\\leq 5.0\\) may be acceptable in a secondary assay, where the data will be used for more categorical decisions (e.g. &gt; 100-fold selectivity vs. the primary assay). If more precision is needed, that can be achieved by increasing the number of times that each sample is independently tested. The MSR calculation for the Replicate-Experiment assumes that most samples tested will only be measured a single time. MSR is dependent upon the number of routine replicates (Haas et al. 2017): \\[MSR = 10^{2\\sqrt{2(s/\\sqrt{n})}}\\]\n\n\n\n\nTable 9: MSR values assuming different numbers of independent replicates.\n\n\n\n\n\n\n\n\n\nn = 1\nn = 2\nn = 3\nn = 4\nn = 5\nn = 6\nn = 7\nn = 8\n\n\n\n\n2.53\n1.93\n1.71\n1.59\n1.51\n1.46\n1.42\n1.39\n\n\n\n\n\n\n\n\n\n\nTable 9 shows the calculated MSR for different numbers of independent replicates. The value of n indicates the number of different experiments each compound should be tested in to reach the calculated MSR.\n\n\n\n\nEarly in a project, it may be a struggle to identify a sufficient number of validated, active samples to perform a replicate experiment. Here is an example where there are only 5 unique samples. Unfortunately, this low sample number does not provide enough statistical power to obtain a good estimate of the assay’s variability, so the Ratio Limits and MSR can appear to be greater than their true values.\n\n\n\n\n\n\nFigure 12: Mean Ratio plot.\n\n\n\n\n\n\n\nTable 10: Replicate Experiment Statistics.\n\n\n\n\n\n\n\n\n\nPotency Stats\n\n\nn\nMeanRatio\nMSR\nUpper Ratio Limit\nLower Ratio Limit\nUpper Agreement Limit\nLower Agreement Limit\n\n\n\n\n5\n1.16\n1.01 × 101\n4.88\n2.75 × 10−1\n2.89 × 101\n4.64 × 10−2\n\n\n\n\n\n\n\n\n\n\nThe Bland-Altman plot, Figure 12, and the associated statistics, Table 10, illustrate the problem. While the MSR appears to be high, we also see that n is only 5, so there is probably not enough data to provide a good estimate of the MSR.\n\n\n\n\n\n\nFigure 13: Correlation Plot\n\n\n\nFortunately, there is a simple solution to this problem. Each sample can be replicated multiple times on the plates. The replicate dose-response curves are then analyzed as if they were from different samples. Ideally, these replicate samples should be prepared independently, as if they were unique compounds with their own dilution series. In this example, the 5 unique samples were each replicated 6 times (e.g. sample 1 becomes 1A, 1B, 1C …). This produces 30 pairs of potency values for the analysis\n\n\n\n\n\n\nFigure 14: Replicated samples Mean Ratio plot.\n\n\n\nThe Bland-Altman plot, Figure 14, now shows 5 clusters of difference values corresponding to the 5 replicated samples.\n\n\n\n\nTable 11: Replicate Experiment Statistics.\n\n\n\n\n\n\n\n\n\nPotency Stats\n\n\nn\nMeanRatio\nMSR\nUpper Ratio Limit\nLower Ratio Limit\nUpper Agreement Limit\nLower Agreement Limit\n\n\n\n\n30\n9.90 × 10−1\n3.53\n1.25\n7.83 × 10−1\n3.59\n2.73 × 10−1\n\n\n\n\n\n\n\n\n\n\nThe associated statistics, Table 11, show a much lower MSR.\n\n\n\n\n\n\nFigure 15: Replicated samples correlation plot.\n\n\n\nSimilar clustering is observed in the correlation plot, Figure 15. While it is easy to identify the clusters due to the sample replication in the graphs, the improvement in statistical power from n=30 instead of n = 5 provides a much better estimate of the true assay variability. Both the MSR and the ratio limits are significantly lower.\n\n\n\nSystematic differences between two experiments could be indicated when the mean ratio line is displaced from the ratio = 1 reference line. If the reference line is outside of the Ratio Limits for the Center Line, the difference is statistically significant and should be investigated to determine the cause. However, if the reference line is within the Ratio Limits, it could simply be random variation or to a cause with a small effect.\n\n\n\n\nTable 12: Replicate Experiment Potency Shift Data.\n\n\n\n\n\n\n\n\n\nPotency Shift Data.\n\n\nSample\nExp1\nExp2\nGeometric_Mean\nRatio\n\n\n\n\n4649104\n2.45\n5.28\n3.60\n4.64 × 10−1\n\n\n2303569\n1.68 × 103\n2.33 × 103\n1.98 × 103\n7.20 × 10−1\n\n\n2002357\n1.01 × 102\n1.05 × 102\n1.03 × 102\n9.62 × 10−1\n\n\n8605428\n1.25 × 101\n5.99 × 101\n2.73 × 101\n2.08 × 10−1\n\n\n6472482\n1.13 × 102\n2.03 × 102\n1.51 × 102\n5.56 × 10−1\n\n\n1210635\n2.04 × 10−3\n3.97 × 10−3\n2.85 × 10−3\n5.13 × 10−1\n\n\n\n\n\n\n\n\n\n\nThe data in Table 12 is used to illustrate this situation.\n\n\n\n\n\n\nFigure 16: Systematic error Bland-Altman plot.\n\n\n\nNow the black reference line (Ratio = 1) is clearly outside of the observed Ratio Limits around the blue Center Line for the data. This indicates that there is a systematic shift in the data between the first and second measurements of the compound potencies.\n\n\n\n\nTable 13: Potency Shift Replicate Experiment Statistics.\n\n\n\n\n\n\n\n\n\nPotency Stats\n\n\nn\nMeanRatio\nMSR\nUpper Ratio Limit\nLower Ratio Limit\nUpper Agreement Limit\nLower Agreement Limit\n\n\n\n\n32\n1.06\n2.53\n1.25\n8.98 × 10−1\n2.73\n4.13 × 10−1\n\n\n\n\n\n\n\n\n\n\nThe associated statistics, Table 13, also shows that the expected ratio of 1 is not within the ratio limits.\n\n\n\n\n\n\nFigure 17: Systematic Shift Correlation Plot.\n\n\n\nThe correlation plot, Figure 17, also shows separation between the reference identity line and the correlation line.\nCommon causes can include any of the following:\n\nDifferences in the samples between experiments (e.g. sample lots, or solubilizations).\nReagents or cells used in the assay.\nEquipment changes.\nEnvironmental changes (e.g. temperature).\nPersonnel\n\nIf a cause is identified, it should be mitigated (if possible) and the replicate experiment repeated. Sometimes a cause may be identified which includes historical data that can’t be changed. If this happens, it may be significant enough to require retesting of key compounds to assess the impact."
  },
  {
    "objectID": "RepExpDoc.html#acknowledgement",
    "href": "RepExpDoc.html#acknowledgement",
    "title": "Replicate Experiment",
    "section": "",
    "text": "The Assay Guidance Manual Webtools are supported by the Intramural Research Program of the National Center for Advancing Translational Sciences at the National Institutes of Health (ZIA TR000340)."
  }
]